{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a430fa6",
   "metadata": {},
   "source": [
    "This notebook runs web scraping of the career seach websites for some companies. It outputs a csv file for each company.\n",
    "\n",
    "By default the script updates the jobs for all companies. If only selected companies are to be updated, list those companies in the python list of 'update_list' which can be found at the end of the script.\n",
    "\n",
    "When a new company is added, the following items need to be updated:\n",
    "1. complete_company_list\n",
    "2. add a function of scrape_jobs_[company_name]() \n",
    "3. add a block to the function of main_update_func(update_list) for the new company\n",
    "\n",
    "Scraping of some websites may fail for various reasons. Often times if you rerun for these companies only using 'update_list' will solve the problem. If not, then one needs to debug and fix the code if necessary (html format may change overtime). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76512914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d9d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24bf65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging, set to -1 to scrape all jobs, otherwise will scrape jobs from index of 0 to test-1\n",
    "test=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c275db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "complete_company_list = ['Accenture',\n",
    "                        'Amazon',\n",
    "                        'Apple',\n",
    "                        'Cisco',\n",
    "                        'Collabera',\n",
    "                        'Deloitte',\n",
    "                        'Expedia',\n",
    "                        'Fox News',\n",
    "                        'Google',\n",
    "                        'IBM',\n",
    "                        'Infosys',\n",
    "                        'Intel',\n",
    "                        'JnJ',\n",
    "                        'JPM',\n",
    "                        'KPMG',\n",
    "                        'Microsoft',\n",
    "                        'Nvidia',\n",
    "                        'Oracle',\n",
    "                        'State Farm',\n",
    "                        'Texas Instruments',\n",
    "                        'Vistra',\n",
    "                        'Vizient',\n",
    "                        'Walmart']\n",
    "\n",
    "total_company_counts = len(complete_company_list)\n",
    "print(total_company_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735c7d2",
   "metadata": {},
   "source": [
    "## Specify the URL for each website (keywords = 'technology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cfd5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accenture_url1 = 'https://www.accenture.com/us-en/careers/jobsearch?jk=technology&sb=0&vw=0&is_rj=0&pg='\n",
    "accenture_url2 = ''\n",
    "\n",
    "amazon_url1 = 'https://www.amazon.jobs/en/search?offset='\n",
    "amazon_url2 = '&result_limit=10&sort=relevant&country%5B%5D=USA&distanceType=Mi&radius=24km&latitude=&longitude=&loc_group_id=&loc_query=&base_query=technology&city=&country=&region=&county=&query_options=&'\n",
    "         \n",
    "deloitte_url = 'https://apply.deloitte.com/careers/SearchJobs/technology?sort=relevancy'\n",
    "\n",
    "google_url1 = 'https://careers.google.com/jobs/results/?page='\n",
    "google_url2 = '&q=technology'\n",
    "    \n",
    "ibm_url = 'https://www.ibm.com/careers/us-en/search/?search=technology&filters=primary_country:CA,primary_country:US'\n",
    "\n",
    "intel_url = 'https://jobs.intel.com/en/search-jobs/technology/599/1' \n",
    "\n",
    "jnj_url1 = 'https://jobs.jnj.com/en/jobs/?page='\n",
    "jnj_url2 = '&search=technology&country=United+States&pagesize=20#results' \n",
    "\n",
    "jpm_url = 'https://jpmc.fa.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1001/requisitions?keyword=software+engineer&location=United+States&locationId=300000000289738&locationLevel=country'\n",
    "    \n",
    "kpmg_url1 = 'https://www.kpmguscareers.com/job-search/?career-level-parents=Experienced%7C&career-level=&spage='\n",
    "kpmg_url2 = ''    \n",
    "\n",
    "microsoft_url1 = 'https://careers.microsoft.com/us/en/search-results?keywords=technology&from='\n",
    "microsoft_url2 = '&s=1'\n",
    "\n",
    "nvidia_url = 'https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite'\n",
    "\n",
    "oracle_url = 'https://eeho.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/requisitions?keyword=technology&location=United+States&locationId=300000000149325&locationLevel=country&mode=location'\n",
    "\n",
    "statefarm_url1 = 'https://jobs.statefarm.com/main/jobs?page='\n",
    "statefarm_url2 = '&keywords=Technology&sortBy=relevance'\n",
    "\n",
    "ti_url1 = 'https://careers.ti.com/search-jobs/?keyword=technology&pg='\n",
    "ti_url2 = ''\n",
    "\n",
    "vistra_url = 'https://vst.wd5.myworkdayjobs.com/en-US/vistra_careers'\n",
    "\n",
    "vizient_url = 'https://vizient.wd1.myworkdayjobs.com/Vizient_Careers'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a76412",
   "metadata": {},
   "source": [
    "### Uncomment the cell below if want to scrape all jobs (no keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f8f3f",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "accenture_url1 = 'https://www.accenture.com/us-en/careers/jobsearch?jk=&sb=1&vw=0&is_rj=0&pg=1'\n",
    "accenture_url2 = ''\n",
    "\n",
    "amazon_url1 = 'https://www.amazon.jobs/en/search?offset='\n",
    "amazon_url2 = '&result_limit=10&sort=relevant&distanceType=Mi&radius=24km&latitude=&longitude=&loc_group_id=&loc_query=&base_query=&city=&country=&region=&county=&query_options=&'\n",
    " \n",
    "deloitte_url = 'https://apply.deloitte.com/careers/SearchJobs?sort=relevancy'\n",
    "\n",
    "google_url1 = 'https://careers.google.com/jobs/results/?page='\n",
    "google_url2 = ''\n",
    "    \n",
    "ibm_url = 'https://www.ibm.com/careers/us-en/search/?filters=primary_country:CA,primary_country:US'\n",
    "   \n",
    "intel_url = 'https://jobs.intel.com/en/search-jobs' \n",
    "\n",
    "jnj_url1 = 'https://jobs.jnj.com/en/jobs/?page='\n",
    "jnj_url2 = '&country=United+States&pagesize=20#results' \n",
    "\n",
    "jpm_url = 'https://jpmc.fa.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1001/requisitions?location=United+States&locationId=300000000289738&locationLevel=country'\n",
    "\n",
    "kpmg_url1 = 'https://www.kpmguscareers.com/job-search/?career-level-parents=Experienced%7C&career-level=&spage='\n",
    "kpmg_url2 = ''    \n",
    "\n",
    "microsoft_url1 = 'https://careers.microsoft.com/us/en/search-results?from='\n",
    "microsoft_url2 = '&s=1'\n",
    "\n",
    "nvidia_url = 'https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite'\n",
    "\n",
    "oracle_url = 'https://eeho.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/requisitions?location=United+States&locationId=300000000149325&locationLevel=country&mode=location'\n",
    "\n",
    "statefarm_url1 = 'https://jobs.statefarm.com/main/jobs?page='\n",
    "statefarm_url2 = '&sortBy=relevance'\n",
    "\n",
    "ti_url1 = 'https://careers.ti.com/search-jobs/?pg='\n",
    "ti_url2 = ''\n",
    "\n",
    "vistra_url = 'https://vst.wd5.myworkdayjobs.com/en-US/vistra_careers'\n",
    "\n",
    "vizient_url = 'https://vizient.wd1.myworkdayjobs.com/Vizient_Careers'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a1292",
   "metadata": {},
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5cf97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(s):\n",
    "  s_new = s.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "  while '  ' in s_new:\n",
    "    s_new = s_new.replace('  ', ' ')\n",
    "  return s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c61d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(company_name, title, link, qual, descrp):\n",
    "    df = pd.DataFrame(zip(title, qual, link, descrp))\n",
    "    df.columns = ['TITLE', 'QUALIFICATIONS', 'LINK', 'DESCRIPTION']\n",
    "    df['COMPANY'] = company_name\n",
    "    df = df.iloc[:, [4, 0, 1, 2, 3]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "921f3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_rows(df, colname) :\n",
    "  # look for empty QUALIFICATION entries\n",
    "  empty_idx=[]\n",
    "  for i in range(len(df[colname])):\n",
    "    count = len(df[colname][i])\n",
    "    if count<=10 : empty_idx.append(i)\n",
    "    else: pass\n",
    "  return empty_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40596292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca38592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_and_ouput(company_name, title, link, qual, descrp):\n",
    "    \n",
    "    qual_cleaned = [remove_chars(q) for q in qual]\n",
    "    descrp_cleaned = [remove_chars(d) for d in descrp]\n",
    "    qual_cleaned = [d if len(q)==0 else q for q,d in zip(qual_cleaned,descrp_cleaned)] \n",
    "    \n",
    "    # create a dataframe from the data\n",
    "    df = make_df(company_name, title, link, qual_cleaned, descrp_cleaned)\n",
    "    #print(df.shape[0])\n",
    "    \n",
    "    # drop the empty Qualification entries\n",
    "    df\n",
    "    df.drop(get_empty_rows(df, 'QUALIFICATIONS'), inplace=True)\n",
    "    \n",
    "    #remove the duplicated jobs\n",
    "    df_nodup = df.drop_duplicates()\n",
    "    print(\"There are {} jobs from {}.\".format(df_nodup.shape[0], company_name))\n",
    "    \n",
    "    filename = company_name + '_technology_jobs_cleaned.csv'\n",
    "    try:\n",
    "        df_nodup.to_csv(filename)\n",
    "    except:\n",
    "        print(f'ERROR: Failed to save the file ({filename})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a12d9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_links_byUrl(title_tag, title_attr, title_value, \n",
    "                           link_tag, link_attr, link_value, link_prefix, \n",
    "                           url1, url2, start=1, multiplier=1):\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "    page_num = start\n",
    "    url = url1 + str(page_num*multiplier) + url2\n",
    "    driver=webdriver.Chrome(options = chrome_options)\n",
    "\n",
    "    try:\n",
    "        soup = get_html(driver, url)\n",
    "    except:\n",
    "        print(f'ERROR: Failed to load {url}.')\n",
    "\n",
    "    # will exit while loop when soup.find_all returns None\n",
    "    while soup.find_all(title_tag, {title_attr: title_value}):\n",
    "        job_title.extend([td.text for td in soup.findAll(title_tag, {title_attr: title_value})])\n",
    "        job_link.extend([link_prefix + td['href'] for td in soup.findAll(link_tag, {link_attr: link_value})])\n",
    "        driver.quit()\n",
    "              \n",
    "        page_num += 1\n",
    "        driver=webdriver.Chrome(options = chrome_options)\n",
    "        url = url1 + str(page_num*multiplier) + url2\n",
    "        \n",
    "        try:\n",
    "            soup = get_html(driver, url)\n",
    "        except:\n",
    "            print(f'ERROR: Failed to load {url}.')\n",
    "\n",
    "    driver.quit()\n",
    "        \n",
    "    df = pd.DataFrame(zip([remove_chars(job) for job in job_title],job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "    df = df.drop_duplicates() \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4afc4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_links_by_btnClick(driver, total_pages, NextBtn, title_tag, title_attr, title_value, \n",
    "                                 link_tag, link_attr, link_value, link_prefix):\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "    \n",
    "    current_page = 1\n",
    "    while current_page <= total_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        job_title.extend([td.text for td in soup.findAll(title_tag, {title_attr: title_value})])\n",
    "        job_link.extend([link_prefix + td['href'] for td in soup.findAll(link_tag, {link_attr: link_value})])\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element('xpath', NextBtn) \n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            current_page += 1\n",
    "        except: break\n",
    "    driver.quit()\n",
    "        \n",
    "    df = pd.DataFrame(zip(job_title,job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "    df = df.drop_duplicates() \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef3133",
   "metadata": {},
   "source": [
    "## Accenture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c40734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accenture_get_jobs(url1, url2, totalPages, jobsPerPage):\n",
    "    \"\"\"\n",
    "    retrieve job titles and job links from each page\n",
    "    \"\"\"\n",
    "    title = []\n",
    "    link = []\n",
    "    page_num = 0\n",
    "\n",
    "    driver=webdriver.Chrome(options = chrome_options)\n",
    "    URL = url1 + str(page_num+1) + url2\n",
    "    try:\n",
    "        soup = get_html(driver, URL)\n",
    "    except:\n",
    "        print(f'ERROR: Failed to load {URL}.')\n",
    "    \n",
    "    soup.find('a', {\"class\" : \"cmp-jobs-results__title\"}).tag\n",
    "    print(totalPages, jobsPerPage)\n",
    "    # loop through all pages\n",
    "    while page_num <= totalPages :\n",
    "        driver.quit()\n",
    "        tags = soup.find_all(\"a\", {\"class\": \"cmp-teaser__title-link\"})[2:jobsPerPage+2]\n",
    "        link.extend(t['href'] for t in tags)\n",
    "        tags = soup.find_all(\"h3\", {\"class\": \"cmp-teaser__title\"})[2:jobsPerPage+2]\n",
    "        title.extend(t.text for t in tags)\n",
    "\n",
    "        page_num += 1\n",
    "        driver=webdriver.Chrome(options = chrome_options)\n",
    "        URL = url1 + str(page_num+1) + url2        \n",
    "        try:\n",
    "            soup = get_html(driver, URL)\n",
    "        except:\n",
    "            print(f'ERROR: Failed to load {URL}.')\n",
    "    \n",
    "    driver.quit() \n",
    "    df = pd.DataFrame(zip(title, link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "    df = df.drop_duplicates() \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b162d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accenture_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]       \n",
    "        try:\n",
    "            soup = get_html(driver, URL)\n",
    "        except:\n",
    "            print(f'ERROR: Failed to load {URL}.')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "        \n",
    "        try:\n",
    "          tags = soup.find('h2', text=\"Qualifications\").findNext('ul').parent.find_all('ul')\n",
    "          for t in tags:\n",
    "            s = s + \" \" + t.text\n",
    "        except: pass\n",
    "\n",
    "        # retrieve job descriptions\n",
    "        try:\n",
    "          d = soup.find('div', {'class': \"description-content\"}).text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00f1615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_accenture():   \n",
    "    # get the first page\n",
    "    driver=webdriver.Chrome(options = chrome_options)\n",
    "    URL = accenture_url1 + \"1\" + accenture_url2\n",
    "    \n",
    "    try:\n",
    "        soup = get_html(driver, URL)\n",
    "    except:\n",
    "        print(f'ERROR: Failed to load {URL}.')\n",
    "    \n",
    "    jobs_per_page = 0\n",
    "    \n",
    "    tags = soup.find_all(\"a\", {\"class\": \"cmp-teaser__title-link\"})\n",
    "    for t in tags:\n",
    "        if \"www.accenture.com/us-en/careers\" in t['href']:\n",
    "            jobs_per_page += 1\n",
    "    \n",
    "    # get total number of jobs and calculate total number of pages\n",
    "    s = soup.find('a', {\"class\" : \"cmp-jobs-results__title\"}).text\n",
    "    total_count = int(s.replace(\")\", \"\").split(\"(\")[1])\n",
    "    total_pages = int(total_count / jobs_per_page + 1)\n",
    "    \n",
    "    df_title_link = accenture_get_jobs(accenture_url1, accenture_url2, total_pages, jobs_per_page)\n",
    "    print(df_title_link.shape[0])\n",
    "    \n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = accenture_job_description(df_title_link['JOB_TITLE'].values[:-2], df_title_link['JOB_LINK'].values[:-2])\n",
    "\n",
    "    post_process_and_ouput('Accenture', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1aacb8",
   "metadata": {},
   "source": [
    "## Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b314fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        try:\n",
    "            soup = get_html(driver, URL)\n",
    "        except:\n",
    "            print(f'ERROR: Failed to load {URL}.')\n",
    "\n",
    "        s = ''\n",
    "        d = ''\n",
    "        try:\n",
    "          tag = soup.find(\"h2\", text='BASIC QUALIFICATIONS').parent\n",
    "          if tag.find('li'):\n",
    "            for t in tag.find_all('li'):\n",
    "              s = s + ' ' + t.text\n",
    "          else:\n",
    "            s = s + ' ' + tag.text\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "          tag = soup.find(\"h2\", text='PREFERRED QUALIFICATIONS').parent\n",
    "          if tag.find('li'):\n",
    "            for t in tag.find_all('li'):\n",
    "              s = s + ' ' + t.text\n",
    "          else:\n",
    "            s = s + ' ' + tag.text\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "          tag = soup.find(\"h2\", text='DESCRIPTION').parent\n",
    "          if tag:\n",
    "            d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "\n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "401545a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_amazon():   \n",
    "    df_title_link = get_titles_links_byUrl('h3', 'class', 'job-title', \n",
    "                                           'a', 'class', 'job-link', 'https://www.amazon.jobs', \n",
    "                                           amazon_url1, amazon_url2, 0, 10)\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = amazon_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Amazon', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb7c9a",
   "metadata": {},
   "source": [
    "## Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f3209f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_apple():  \n",
    "    x = 1\n",
    "    i = 1\n",
    "    job_title = []\n",
    "    job_link = []\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(20)\n",
    "\n",
    "    url = 'https://www.apple.com/careers/us/'\n",
    "\n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    searchButton = driver.find_element(By.XPATH, \"//a[text()='Search']\")\n",
    "    searchButton.click()\n",
    "\n",
    "    searchTextBox = driver.find_element(By.XPATH, \"//input[@placeholder='Search by role or keyword']\")\n",
    "    searchTextBox.send_keys(\"Software Engineer\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    searchTextBox.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)\n",
    "\n",
    "    try:\n",
    "\n",
    "        for x in range(5):\n",
    "\n",
    "            y = 0\n",
    "            while y != 20:\n",
    "                title = driver.find_element(By.XPATH, \"(//tbody//a[contains(@id,'jotTitle')])[\" + str(y + 1) + \"]\").text\n",
    "                job_title.append(title)\n",
    "\n",
    "                link = driver.find_element(By.XPATH, \"(//td[@class='table-col-1']//a[@class][@href])[\" + str(y + 1) + \"]\")\n",
    "                job_link.append(link.get_attribute('href'))\n",
    "                y = y + 1\n",
    "\n",
    "            driver.find_element(By.XPATH, \"//span[@class='next']\").click()\n",
    "            x=x+1\n",
    "\n",
    "    except NoSuchElementException:...\n",
    "\n",
    "    driver.close()    \n",
    "\n",
    "    df = pd.DataFrame(zip(job_title, job_link))\n",
    "    df.columns = ['Title', 'Link']\n",
    "\n",
    "    df['Qualifcations'] = np.nan\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(20)\n",
    "\n",
    "    for i in range(len(df['Link'])):\n",
    "\n",
    "            url = (df['Link'][i])       \n",
    "            driver.get(url)\n",
    "            driver.maximize_window()\n",
    "            time.sleep(3)\n",
    "\n",
    "            qualificationList = driver.find_element(By.XPATH, \"(//ul[@class='jd__list'])[1]\")\n",
    "            qualificationsearch = qualificationList.find_elements(By.TAG_NAME, \"li\")\n",
    "            df.loc[i, 'Qualifcations'] = ''\n",
    "\n",
    "            for y in qualificationsearch:\n",
    "                df['Qualifcations'][i] = df['Qualifcations'][i]+' '+y.text  \n",
    "\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    to_drop = df[df['Qualifcations'] == ''].index\n",
    "    df = df.drop(to_drop)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['Company'] = 'Apple'\n",
    "    df = df[['Company', 'Title', 'Link', 'Qualifcations']]\n",
    "    df.head()\n",
    "    df.to_csv('apple_jobs.csv')\n",
    "    print(\"There are {} jobs from Apple.\".format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17937f9a",
   "metadata": {},
   "source": [
    "## Cisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfeb9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_cisco():\n",
    "\n",
    "    job_title = []\n",
    "    job_link = []\n",
    "\n",
    "    x = 1\n",
    "    i = 0\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    while x > 0:\n",
    "        try:\n",
    "            url = 'https://jobs.cisco.com/jobs/SearchJobs/?21178=%5B169482%5D&21178_format=6020&listFilterMode=1&projectOffset='+str(i*25)    \n",
    "            driver.get(url)\n",
    "\n",
    "            #table = driver.find_element(By.XPATH, '//*[@id=\"content\"]/div/div[2]/table')\n",
    "\n",
    "            for j in range(1,25):\n",
    "                job_title.append(driver.find_element(By.XPATH, '//*[@id=\"content\"]/div/div[2]/table/tbody/tr['+str(j)+']/td[1]/a').text)       \n",
    "                job_link.append(driver.find_element(By.XPATH, '//*[@id=\"content\"]/div/div[2]/table/tbody/tr['+str(j)+']/td[1]/a').get_attribute('href'))\n",
    "\n",
    "            i += 1\n",
    "        except:\n",
    "            print('last page scraped: ', i)\n",
    "            x -= 1 \n",
    "\n",
    "    driver.close()\n",
    "    print(len(job_title))\n",
    "\n",
    "    df = pd.DataFrame(zip(job_title, job_link))\n",
    "    df.columns = ['TITLE', 'LINK']\n",
    "\n",
    "    df['QUALIFICATIONS'] = np.nan\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for i in range(len(df['LINK'])):\n",
    "        try:\n",
    "            url = (df['LINK'][i])       \n",
    "            driver.get(url)\n",
    "\n",
    "            txt = driver.find_element(By.XPATH, '//*[@id=\"content\"]/div/div[2]/div/div[2]/div[3]/div/div[1]')\n",
    "            qual = txt.find_elements(By.TAG_NAME, 'li')\n",
    "            df.loc[i,'QUALIFICATIONS'] = ''\n",
    "\n",
    "            for q in qual:\n",
    "                df.loc[i,'QUALIFICATIONS'] = df['QUALIFICATIONS'][i]+' '+q.text  \n",
    "        except:\n",
    "            df.loc[i,'QUALIFICATIONS'] = np.nan \n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    to_drop = df[df['QUALIFICATIONS'] == ''].index\n",
    "    df = df.drop(to_drop)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    df['COMPANY'] = 'Cisco'\n",
    "    #df['DESCRIPTION'] = df['QUALIFICATIONS']\n",
    "    df['DESCRIPTION'] = ''\n",
    "    df = df[['COMPANY', 'TITLE', 'QUALIFICATIONS', 'LINK', 'DESCRIPTION']]\n",
    "    print(\"There are {} jobs from Cisco.\".format(df.shape[0]))\n",
    "\n",
    "    df.to_csv('cisco_jobs_usa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442230e",
   "metadata": {},
   "source": [
    "## Collabera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30ee76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_collabera():\n",
    "    # loop doesn't break. Once the pages end, it automatically loads the last page.\n",
    "    x = 1\n",
    "    i = 1\n",
    "\n",
    "    job_title = []\n",
    "    job_link = []\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(20)\n",
    "\n",
    "    while x != 0:\n",
    "        try:\n",
    "            url = 'https://collabera.com/job-search/?sort_by=&industry=&keyword=&location=&Posteddays=0&q='+str(i)\n",
    "\n",
    "            driver.get(url)\n",
    "\n",
    "            for j in range(1,11):\n",
    "                title = driver.find_element(By.XPATH,'/html/body/div[1]/section[3]/div/div/div[2]/div/div[2]/div['+str(j)+']/div/a/h5')\n",
    "                job_title.append(title.text)\n",
    "\n",
    "                link = driver.find_element(By.XPATH,'/html/body/div[1]/section[3]/div/div/div[2]/div/div[2]/div['+str(j)+']/div/a')\n",
    "                job_link.append(link.get_attribute('href'))    \n",
    "        except:\n",
    "            x -= 1\n",
    "        i +=1\n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "    print(len(job_title))\n",
    "    \n",
    "    df = pd.DataFrame(zip(job_title, job_link))\n",
    "    df.columns = ['TITLE', 'LINK']\n",
    "    df['QUALIFICATIONS'] = np.nan\n",
    "\n",
    "    # Inconsistant formatting/wording on the job description page.\n",
    "    # I tested getting just the li tags. However, the amount of text is very similar.\n",
    "    # But since some job description pages don't have li tags, those will be missed by the script.\n",
    "    # Therefore, I decided to get the whole text on the job description page. Luckily, the text is not very long on the pgaes.\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for i in range(len(df['LINK'])):\n",
    "\n",
    "        try:\n",
    "            url = (df['LINK'][i]) \n",
    "            driver.get(url)             \n",
    "            txt = driver.find_element(By.XPATH, '/html/body/div[1]/section[2]/div/div/div[1]/div[1]/div')\n",
    "            df.loc[i,'QUALIFICATIONS'] = txt.text\n",
    "\n",
    "        except:\n",
    "            df.loc[i,'QUALIFICATIONS'] = np.nan\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    df = df.dropna()\n",
    "    df[df['QUALIFICATIONS'] == '']\n",
    "\n",
    "    df['COMPANY'] = 'Collabera'\n",
    "    #df['DESCRIPTION'] = df['QUALIFICATIONS']\n",
    "    df['DESCRIPTION'] = ''\n",
    "    df = df.reindex(columns=['COMPANY', 'TITLE', 'QUALIFICATIONS', 'LINK', 'DESCRIPTION'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"There are {} jobs from Collabera.\".format(df.shape[0]))\n",
    "    \n",
    "    df.to_csv('collabera_jobs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187bcfc8",
   "metadata": {},
   "source": [
    "## Deloitte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "731e3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deloitte_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    qual_pattern = re.compile(\"qualifications|required:\", re.IGNORECASE)\n",
    "    descrp_pattern = re.compile(\"work you’ll do|job duties\", re.IGNORECASE)\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        try:\n",
    "            soup = get_html(driver, URL)\n",
    "        except:\n",
    "            print(f'ERROR: Failed to load {URL}.')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "        \n",
    "        # get descriptions\n",
    "        try:\n",
    "          tag = soup.find(\"strong\", text=descrp_pattern).findNext(\"ul\")\n",
    "          d = tag.text\n",
    "        except: pass\n",
    "    \n",
    "        # get qualifications\n",
    "        try:\n",
    "          tag = soup.find(re.compile(\"(strong|span)\"), text=qual_pattern).findNext(\"ul\")\n",
    "          s = tag.text\n",
    "          if tag.findNext(\"ul\"):\n",
    "            s = s + \" \" + tag.findNext('ul').text\n",
    "        except: pass\n",
    "                      \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1b0b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_deloitte():\n",
    "    # go to the first page of job post\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(deloitte_url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # load all jobs to the current page by clicking the load more button\n",
    "    next_button = driver.find_element('xpath', '//*[@class=\"button button--default button--loadmore\"]')\n",
    "    while next_button:\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            next_button = driver.find_element('xpath', '//*[@class=\"button button--default button--loadmore\"]')\n",
    "        except:\n",
    "            break\n",
    "\n",
    "     # scrape all job titles and links\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    job_title.extend([td.text for td in soup.find_all(\"a\", {\"class\": \"link\"})])\n",
    "    job_link.extend([td['href'] for td in soup.find_all(\"a\", {\"class\": \"link\"})])   \n",
    "\n",
    "    # remove unwanted chars\n",
    "    job_title_cleaned = [remove_chars(s) for s in job_title]\n",
    "    job_link_cleaned = [remove_chars(s) for s in job_link]\n",
    "\n",
    "    # create a dataframe that contains job titles and links for all job categories\n",
    "    df_title_link = pd.DataFrame(zip(job_title_cleaned, job_link_cleaned), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_title_link = df_title_link.drop_duplicates()\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = deloitte_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Deloitte', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349230c",
   "metadata": {},
   "source": [
    "## Expedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d8e0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_expedia():\n",
    "    job_title = []\n",
    "    job_link = []\n",
    "\n",
    "    #this url redirects to the page 'https://sjobs.brassring.com/TGnewUI/Search/Home/Home?partnerid=25633&siteid=5439&Codes=BeMore#home'\n",
    "    #url = 'https://sjobs.brassring.com/TGnewUI/Search/Home/Home?partnerid=25633&siteid=5439&Codes=BeMore#keyWordSearch=technology%20or%20software%20engineering%20or%20developer%20or%20azure%20or%20aws&locationSearch='\n",
    "\n",
    "    url = 'https://careers.expediagroup.com/jobs/?&filter[country]=United+States'\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.implicitly_wait(20)\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    x = 1\n",
    "    i = 1\n",
    "\n",
    "    while x == 1:\n",
    "        try:\n",
    "\n",
    "            next_button = driver.find_element(By.ID,'loadmore')\n",
    "            next_button.click()\n",
    "\n",
    "        except:\n",
    "            print('page',i)\n",
    "            x = 0 \n",
    "        i +=1\n",
    "\n",
    "    TITLE = []\n",
    "    LINK = []\n",
    "\n",
    "    y = 1     \n",
    "    i = 1\n",
    "\n",
    "    while y != 0:\n",
    "    #while i < 3:\n",
    "\n",
    "        y = 2\n",
    "\n",
    "        try:\n",
    "            j_title = driver.find_element(By.XPATH,'//*[@id=\"resultslist\"]/li['+str(i)+']/a/h3')\n",
    "            TITLE.append(j_title.text) \n",
    "        except:\n",
    "            TITLE.append('')\n",
    "            y-=1\n",
    "\n",
    "        try:\n",
    "            j_link = driver.find_element(By.XPATH,'//*[@id=\"resultslist\"]/li['+str(i)+']/a')\n",
    "            LINK.append(j_link.get_attribute('href'))\n",
    "        except:\n",
    "            LINK.append('')\n",
    "            y-=1\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "    # convert to dataframe\n",
    "\n",
    "    df = pd.DataFrame(zip(TITLE, LINK))\n",
    "    df.columns = ['TITLE', 'LINK']\n",
    "    print(df.shape[0])\n",
    "    df.to_csv('Expedia_title_link.csv')\n",
    "    df = pd.read_csv('Expedia_title_link.csv', index_col=0)\n",
    "\n",
    "    df = df.dropna()\n",
    "    #df.isna().sum()\n",
    "\n",
    "    df['QUALIFICATIONS'] = np.nan\n",
    "    df['DESCRIPTION'] = np.nan\n",
    "    df.drop_duplicates(subset=['TITLE', 'LINK'])\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for i in range(len(df['LINK'])):\n",
    "        url = (df['LINK'][i])\n",
    "        #print(i, url)\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            qual = driver.find_element(By.XPATH,'/html/body/main/div[2]/div/div/div[1]/section[3]/div/div/div/div[1]/ul[2]')\n",
    "            df.loc[i,'QUALIFICATIONS'] = qual.text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            desc = driver.find_element(By.XPATH,'/html/body/main/div[2]/div/div/div[1]/section[3]/div/div/div/div[1]/ul[1]')\n",
    "            df.loc[i,'DESCRIPTION'] = desc.text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    df['QUALIFICATIONS'] = df['QUALIFICATIONS'].str.lower()\n",
    "    df['DESCRIPTION'] = df['DESCRIPTION'].str.lower()\n",
    "    df = df.drop_duplicates(subset=['TITLE', 'QUALIFICATIONS', 'DESCRIPTION'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.reindex(columns=['COMPANY', 'TITLE', 'QUALIFICATIONS', 'LINK', 'DESCRIPTION'])\n",
    "    df['COMPANY'] = 'Expedia'\n",
    "    \n",
    "    print(\"There are {} jobs from Expedia.\".format(df.shape[0]))\n",
    "    df.to_csv('expedia_jobs_usa.csv')\n",
    "    os.remove('Expedia_title_link.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842765d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb13d29",
   "metadata": {},
   "source": [
    "## Fox News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84e25e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve job qualifications and descriptions\n",
    "def fox_job_description_2(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "\n",
    "        text_pattern_descrp = re.compile(\"A SNAPSHOT OF YOUR RESPONSIBILITIES|JOB DESCRIPTION\", re.IGNORECASE)\n",
    "        #text_pattern_descrp_2 = re.compile(\"JOB DESCRIPTION\", re.IGNORECASE)\n",
    "        \n",
    "        \n",
    "        text_pattern_qual = re.compile(\"WHAT YOU WILL NEED|\\\n",
    "                                        WHAT YOU NEED|\\\n",
    "                                        JOB RELATED KNOWLEDGE, SKILLS AND ABILITIES:|\\\n",
    "                                        ABOUT YOU\", re.IGNORECASE)\n",
    "        #text_pattern_qual_2 = re.compile(\"WHAT YOU NEED\", re.IGNORECASE)\n",
    "        #text_pattern_qual_3 = re.compile(\"JOB RELATED KNOWLEDGE, SKILLS AND ABILITIES:\", re.IGNORECASE)\n",
    "        #text_pattern_qual_4 = re.compile(\"ABOUT YOU\", re.IGNORECASE)\n",
    "        text_pattern_qual_5 = re.compile(\"QUALIFICATIONS\", re.IGNORECASE)        \n",
    "        \n",
    "        #trys all of the qualification metrics\n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_qual).findNext(\"ul\")\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_qual_5).findNext(\"p\")\n",
    "            s = s + tag.text\n",
    "        except: pass               \n",
    "        #trys all of the description metrics\n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_descrp).findNext(\"ul\")\n",
    "            d = tag.text\n",
    "        except: \n",
    "            try:\n",
    "                tag = soup.find(\"b\", text = text_pattern_descrp).findNext(\"br\")\n",
    "                s = s + tag.text\n",
    "            except: pass        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00967986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve job qualifications and descriptions\n",
    "def fox_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "\n",
    "        text_pattern_descrp_1 = re.compile(\"A SNAPSHOT OF YOUR RESPONSIBILITIES\", re.IGNORECASE)\n",
    "        text_pattern_descrp_2 = re.compile(\"JOB DESCRIPTION\", re.IGNORECASE)\n",
    "        \n",
    "        \n",
    "        text_pattern_qual_1 = re.compile(\"WHAT YOU WILL NEED\", re.IGNORECASE)\n",
    "        text_pattern_qual_2 = re.compile(\"WHAT YOU NEED\", re.IGNORECASE)\n",
    "        text_pattern_qual_3 = re.compile(\"JOB RELATED KNOWLEDGE, SKILLS AND ABILITIES:\", re.IGNORECASE)\n",
    "        text_pattern_qual_4 = re.compile(\"ABOUT YOU\", re.IGNORECASE)\n",
    "        text_pattern_qual_5 = re.compile(\"QUALIFICATIONS\", re.IGNORECASE)        \n",
    "        \n",
    "        #trys all of the qualification metrics\n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_qual_1).findNext(\"ul\")\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            tag = soup.find(\"b\", text = text_patter_qual_2).findNext(\"ul\")\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            tag = soup.find(\"b\", text = text_patter_qual_3).findNext(\"ul\")\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            tag = soup.find(\"b\", text = text_patter_qual_4).findNext(\"ul\")\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            tag = soup.find(\"b\", text = text_patter_qual_5).findNext(\"br\")\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "        \n",
    "        \n",
    "               \n",
    "        #trys all of the description metrics\n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_descrp_1).findNext(\"ul\")\n",
    "            d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_descrp_2).findNext(\"ul\")\n",
    "            d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_descrp_2).findNext(\"p\")\n",
    "            d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d00b4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_fox():\n",
    "    # specify the url strings for the company's job posting website\n",
    "    url = 'https://www.foxcareers.com/Search/SearchResults'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # get job titles and links for each page and click the next button to go to the next page until no more\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "\n",
    "    next_button = driver.find_element('xpath', '//*[@id=\"loadMoreButton\"]')  \n",
    "    while next_button:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        job_title.extend([td.text for td in soup.findAll(\"a\", {\"class\": \"searchResultTitle\"})])\n",
    "        job_link.extend(['https://www.foxcareers.com' + td['href'] for td in soup.findAll(\"a\", {\"class\": \"searchResultTitle\"})])\n",
    "        try:\n",
    "            next_button.click()\n",
    "            time.sleep(1)\n",
    "        except: break\n",
    "            \n",
    "     # create a dataframe that contains job titles and links for all job categories\n",
    "    df_title_link = pd.DataFrame(zip(job_title, job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_title_link = df_title_link.drop_duplicates()\n",
    "    print(df_title_link.shape[0])\n",
    "    \n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = fox_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Fox News', title, link, qual, descrp)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6340c69",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0ac54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        s = ''\n",
    "        d = ''\n",
    "        \n",
    "        soup = get_html(driver, link[i])\n",
    "        try:\n",
    "          tag = soup.find(\"h3\", text='Minimum qualifications:').parent.find(\"ul\")\n",
    "          if tag:\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "          tag = soup.find('h3', text='Preferred qualifications:').parent.find(\"ul\").findNextSibling('ul')\n",
    "          if tag:\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "          tag = soup.find(\"div\", {'id': 'accordion-responsibilities'})\n",
    "          if tag:\n",
    "            d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "530736fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_google():\n",
    "    # retrieve job titles and job links\n",
    "    df_title_link = get_titles_links_byUrl('h2', 'class', 'gc-card__title gc-heading gc-heading--beta', \n",
    "                                           'a', 'class', 'gc-card', 'https://careers.google.com', \n",
    "                                           google_url1, google_url2)\n",
    "    print(df_title_link.shape[0])\n",
    "    \n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = google_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Google', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb326d6",
   "metadata": {},
   "source": [
    "## IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5efb5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "    \n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        try:\n",
    "            soup = get_html(driver, URL)\n",
    "        except:\n",
    "            print(f'ERROR: Failed to load {URL}.')\n",
    "\n",
    "        s = ''\n",
    "        d = ''\n",
    "              \n",
    "        # get the job qualifications\n",
    "        try:\n",
    "            tag1 = soup.find(\"span\", text=\"Required Technical and Professional Expertise\")\n",
    "            tag = tag1.findNext(['ul', 'ol'])\n",
    "            s = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            tag2 = soup.find(\"span\", text=\"Preferred Technical and Professional Expertise\")\n",
    "            tag = tag1.findNext(['ul', 'ol'])\n",
    "            s = s + \". \" + tag.text\n",
    "        except: pass\n",
    "\n",
    "        # get the job responsibilities\n",
    "        try:\n",
    "            tag1 = soup.find(\"span\", text=\"Your Role and Responsibilities\")\n",
    "            tag = tag1.findNext(['ul', 'ol'])\n",
    "            d = tag.text\n",
    "        except: pass        \n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4affd287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_ibm():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(ibm_url)\n",
    "    driver.implicitly_wait(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # handle the cookies message window\n",
    "    try:\n",
    "        next_button = driver.find_element('xpath', '//*[@id=\"truste-consent-button\"]')  \n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # we first get the total number of jobs and number of jobs per page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    result = soup.find(\"div\", {\"class\": \"UpperList_quantityJobs__eDIK8\"}).text\n",
    "    jobs_per_page = int(result.split()[1])\n",
    "    total_jobs = int(result.split()[3])\n",
    "\n",
    "    # now calculate the total number of pages\n",
    "    total_pages = total_jobs//jobs_per_page + 1    \n",
    "\n",
    "    # retrieve job titles and job links\n",
    "    df_title_link = get_titles_links_by_btnClick(driver, total_pages, '//*[@aria-labelledby=\"tooltip-6\"]', \n",
    "                                 'h3', 'class', 'bx--card__heading', \n",
    "                                 'a', 'class', 'cds--link bx--card__footer undefined', '')  \n",
    "    print(df_title_link.shape[0])\n",
    "    \n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = ibm_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('IBM', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8806d",
   "metadata": {},
   "source": [
    "## Infosys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10671e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_infosys():\n",
    "    url = 'https://sjobs.brassring.com/TGnewUI/Search/Home/Home?partnerid=25633&siteid=5439&Codes=BeMore#home'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(20)\n",
    "    driver.get(url)\n",
    "\n",
    "    location = driver.find_element(By.XPATH,'//*[@id=\"initialSearchBox__26\"]')\n",
    "    location.send_keys('usa')\n",
    "\n",
    "    search_button = driver.find_element(By.XPATH,'//*[@id=\"searchControls_BUTTON_2\"]')\n",
    "    search_button.click()\n",
    "\n",
    "    x = 1\n",
    "    while x == 1:\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH,'//*[@id=\"showMoreJobs\"]')\n",
    "            next_button.click()\n",
    "        except:\n",
    "            x = 0    \n",
    "\n",
    "    job_title = []\n",
    "    job_link = []\n",
    "\n",
    "    y = 2\n",
    "    i = 1\n",
    "\n",
    "    while y != 0:  \n",
    "        y = 2\n",
    "        try:  \n",
    "            job = driver.find_element(By.XPATH,'//*[@id=\"mainJobListContainer\"]/div/div/ul/li['+str(i)+']/div[2]/div[1]')        \n",
    "            job_title.append(job.text)\n",
    "        except:\n",
    "            job_title.append('')\n",
    "            y -= 1\n",
    "\n",
    "        try:\n",
    "            link = driver.find_element(By.XPATH,'//*[@id=\"Job_'+str(i)+'\"]')        \n",
    "            job_link.append(link.get_attribute('href'))\n",
    "        except:\n",
    "            job_link.append('')\n",
    "            y -= 1        \n",
    "\n",
    "        i = i+1\n",
    "\n",
    "    # convert to dataframe\n",
    "\n",
    "    df = pd.DataFrame(zip(job_title, job_link))\n",
    "    df.columns = ['TITLE', 'LINK']\n",
    "    df.head()\n",
    "\n",
    "    df['QUALIFICATIONS'] = np.nan\n",
    "\n",
    "    driver = webdriver.Chrome()   \n",
    "\n",
    "    for i in range(len(df['LINK'])):\n",
    "        try:\n",
    "            job_text = ''\n",
    "            url = (df['LINK'][i])\n",
    "\n",
    "            driver.get(url)\n",
    "\n",
    "            desc = driver.find_element(By.XPATH,'//*[@id=\"content\"]/div[1]/div[7]/div[4]/div[2]/div/div[3]/div[4]/p[2]')\n",
    "            texts = desc.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "            for Text in texts: \n",
    "                job_text = job_text+Text.text+' '\n",
    "\n",
    "            df.loc[i, 'QUALIFICATIONS'] = job_text\n",
    "\n",
    "        except:\n",
    "            df.loc[i, 'QUALIFICATIONS'] = ''\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    to_drop = df[df['QUALIFICATIONS'] == ''].index\n",
    "    df = df.drop(to_drop)\n",
    "    df = df.dropna()\n",
    "\n",
    "    df['QUALIFICATIONS'] = df['QUALIFICATIONS'].str.lower()\n",
    "    df = df.drop_duplicates(subset=['TITLE', 'QUALIFICATIONS'])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    df['COMPANY'] = 'Infosys'\n",
    "    df['DESCRIPTION'] = ''\n",
    "\n",
    "    df = df.reindex(columns=['COMPANY', 'TITLE', 'QUALIFICATIONS', 'LINK', 'DESCRIPTION'])\n",
    "    print(\"There are {} jobs from Infosys.\".format(df.shape[0]))\n",
    "    df.to_csv('infosys_usa_ jobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd43b90",
   "metadata": {},
   "source": [
    "## Intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eeb7e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intel_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "    \n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "\n",
    "        # get job qualifications\n",
    "        # retrieve the text between \"Qualifications\" and the next section with tag \"h2\"\n",
    "        try:\n",
    "            tag = soup.find('h2', text='Qualifications').findNextSibling()\n",
    "            ul=0\n",
    "            space_count = 0\n",
    "            while (tag.name!='h2') & (ul<2):\n",
    "                if tag.name == 'ul':\n",
    "                    ul += 1\n",
    "                s = s + \" \" + tag.text\n",
    "                space_count += 1\n",
    "                try: tag = tag.findNextSibling() \n",
    "                except: pass\n",
    "            \n",
    "            # if there are no 'ul' or other text blocks found, then look for text under tag 'br'\n",
    "            if len(s)<=space_count : \n",
    "                try:\n",
    "                    tag = soup.find('h2', text='Qualifications').findNextSibling('br')\n",
    "                    s = s + \" \" + tag.next_element\n",
    "                except: pass\n",
    "                    \n",
    "        except: pass\n",
    "\n",
    "        # get job descriptions\n",
    "        # retrieve the text between \"Job Description\" and the next section with tag \"h2\"\n",
    "        try:\n",
    "            tag = soup.find('h2', text='Job Description').findNextSibling()\n",
    "            ul=0\n",
    "            space_count = 0\n",
    "            while (tag.name!='h2'):\n",
    "                d = d + \" \" + tag.text\n",
    "                space_count += 1\n",
    "                try: tag = tag.findNextSibling() \n",
    "                except: pass\n",
    "            \n",
    "            # if there are no 'ul' or other text blocks found, then look for text under tag 'br'\n",
    "            if len(s)<=space_count : \n",
    "                try:\n",
    "                    tag = soup.find('h2', text='Job Description').findNextSibling('br')\n",
    "                    d = d + \" \" + tag.next_element\n",
    "                except: pass\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "\n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2e0b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_intel():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(2)\n",
    "    driver.get(intel_url)\n",
    "\n",
    "    # handle the pop up cookies message\n",
    "    try:\n",
    "        next_button = driver.find_element('xpath', '//*[@id=\"igdpr-button\"]')\n",
    "        next_button.click()\n",
    "    except: pass\n",
    "\n",
    "    # get job titles and links for jobs on each page\n",
    "    # click button to go to the next page\n",
    "\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "\n",
    "    while True:\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        tags = soup.find_all('a', {'class': 'job-title-link'})\n",
    "        job_link.extend(['https://jobs.intel.com' + t['href'] for t in tags])\n",
    "        job_title.extend([t.find('h2').text for t in tags])\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element('xpath', '//*[@class=\"next\"]')\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        except: break\n",
    "\n",
    "    job_title = [remove_chars(job) for job in job_title]\n",
    "\n",
    "    # create a dataframe that contains job titles and links for all job categories\n",
    "    df_title_link = pd.DataFrame(zip(job_title, job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_title_link = df_title_link.drop_duplicates()\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = intel_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Intel', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e536e27",
   "metadata": {},
   "source": [
    "## JnJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96961120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jnj_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "    \n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "        \n",
    "        text_pattern = text=re.compile(\"qualifications\", re.IGNORECASE)\n",
    "       \n",
    "        # get job qualifications\n",
    "        try:\n",
    "          tags = soup.find('h3', text=text_pattern).parent.find_all(['ul', 'ol'])\n",
    "          if len(tags) > 0:\n",
    "                for t in tags:\n",
    "                    s = s + \" \" + t.text\n",
    "          else:\n",
    "              try:\n",
    "                tags = soup.find('', text=text_pattern).parent.findNextSiblings(['p'])\n",
    "                for t in tags:\n",
    "                  s = s + \" \" + t.text\n",
    "              except: pass\n",
    "        except: pass\n",
    "            \n",
    "        # get job descriptions\n",
    "        try:\n",
    "          tag = soup.find(\"h2\", text='Description').findNext(['ul', 'ol'])\n",
    "          d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8ce0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_jnj():   \n",
    "    # retrieve job titles and job links\n",
    "    df_title_link = get_titles_links_byUrl('a', 'class', 'stretched-link js-view-job', \n",
    "                                           'a', 'class', 'stretched-link js-view-job', 'https://jobs.jnj.com', \n",
    "                                           jnj_url1, jnj_url2)\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = jnj_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('JnJ', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedfd1d",
   "metadata": {},
   "source": [
    "## JPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b257e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpm_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "    \n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        d = ''\n",
    "              \n",
    "        # get the job responsibilities\n",
    "        try:\n",
    "            tag = soup.find(\"div\", {'class': \"job-description\"})\n",
    "            tags = tag.find_all('ul')\n",
    "            if len(tags) > 0:\n",
    "                for t in tags:\n",
    "                    d = d + \" \" + t.text\n",
    "        except: pass\n",
    "        \n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, description, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73b717dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_jpm():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(jpm_url)\n",
    "\n",
    "    cookie_button = driver.find_element('xpath', '//*[@class=\"cookie-consent__button cookie-consent__button--primary\"]')\n",
    "    cookie_button.click()\n",
    "\n",
    "    #have to scroll up and down several times to make load more button visible\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    next_button = driver.find_element('xpath', '//*[@class=\"search-results-load-more-btn\"]')\n",
    "\n",
    "    while next_button:\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            next_button = driver.find_element('xpath', '//*[@class=\"search-results-load-more-btn\"]')\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    # scrape job titles and links\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    job_title.extend([td.text for td in soup.find_all(\"h3\", {\"class\": \"job-title\"})])\n",
    "    job_link.extend([td['href'] for td in soup.find_all(\"a\", {\"class\": \"joblist-tile\"})])  \n",
    "\n",
    "    # create a dataframe that contains job titles and links for all job categories\n",
    "    df_title_link = pd.DataFrame(zip(job_title, job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_title_link = df_title_link.drop_duplicates()\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = jpm_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('JPM', title, link, qual, descrp)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1254d",
   "metadata": {},
   "source": [
    "## KPMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38f024ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpmg_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "\n",
    "        try:\n",
    "          tag = soup.find(\"h3\", text='Qualifications:').findNextSibling(\"ul\").find_all('li')\n",
    "          if tag:\n",
    "            for t in tag:\n",
    "                s = s + '. ' + t.text\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "          tag = soup.find(\"h3\", text='Responsibilities:').findNextSibling(\"ul\").find_all('li')\n",
    "          if tag:\n",
    "            for t in tag:\n",
    "                d = d + '. ' + t.text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a90c84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_kpmg():   \n",
    "    # retrieve job titles and job links\n",
    "    df_title_link = get_titles_links_byUrl('div', 'class', 'h5 text-dark-grey', \n",
    "                                           'a', 'class', 'box-shadow d-block', 'https://www.kpmguscareers.com', \n",
    "                                           kpmg_url1, kpmg_url2)\n",
    "    print(df_title_link.shape[0])\n",
    "    \n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = kpmg_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('KPMG', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3df03",
   "metadata": {},
   "source": [
    "## Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ddd439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def microsoft_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        s = ''\n",
    "        d = ''\n",
    "        \n",
    "        soup = get_html(driver, link[i])\n",
    "        try:\n",
    "          tag = soup.find('p', {'data-ph-at-id' : 'job-qualifications-text'}).find_all('li')\n",
    "          for t in tag:\n",
    "            s = s + '. ' + t.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "          tag = soup.find('p', {'data-ph-at-id' : 'job-responsibilities-text'})\n",
    "          d =  tag.text\n",
    "        except: pass\n",
    "        \n",
    "        if len(s) > 10:\n",
    "            qualifications.append(s)\n",
    "            description.append(d)   \n",
    "            jobtitle.append(title[i])\n",
    "            joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77c42af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_microsoft():\n",
    "    page_num = 0\n",
    "    url = microsoft_url1 + str(page_num*20) + microsoft_url2\n",
    "    driver=webdriver.Chrome()\n",
    "    soup = get_html(driver, url)\n",
    "    next_button = driver.find_element('xpath', '//*[@class=\"btn primary-button btn-lg phs-search-submit au-target\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs_per_page = len(soup.find_all('span', {'class': 'job-title'})) \n",
    "    total_jobs = int(soup.find('span', {'class': 'total-jobs'}).text)\n",
    "    total_pages = total_jobs//jobs_per_page + 1    \n",
    "    #print(jobs_per_page, total_jobs, total_pages)\n",
    "\n",
    "    # retrieve job titles and job links\n",
    "    df_title_link = get_titles_links_by_btnClick(driver, total_pages, '//a[@aria-label=\"View Next page\"]', \n",
    "                                 'span', 'class', 'job-title', \n",
    "                                 'a', 'data-ph-at-id', 'job-link', '')   \n",
    "    print(df_title_link.shape[0])\n",
    "    \n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = microsoft_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Microsoft', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313af4bf",
   "metadata": {},
   "source": [
    "## Nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12c0436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvidia_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        try:\n",
    "            soup = get_html(driver, URL)\n",
    "        except:\n",
    "            print(f'ERROR: Failed to load {URL}.')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "\n",
    "        text_pattern_descrp = re.compile(\"ll be doing\", re.IGNORECASE)\n",
    "        text_pattern_qual_1 = re.compile(\"what we need to see\", re.IGNORECASE)        \n",
    "        text_pattern_qual_2 = re.compile(\"ways to stand out from the crowd\", re.IGNORECASE)  \n",
    "        \n",
    "        try:\n",
    "          tag = soup.find(\"b\", text=text_pattern_qual_1).findNext(\"ul\")\n",
    "          s = s + tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "          tag = soup.find('b', text=text_pattern_qual_2).findNext(\"ul\")\n",
    "          s = s + tag.text\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "          tag = soup.find(\"b\", text=text_pattern_descrp).findNext(\"ul\")\n",
    "          d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8c2cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_nvidia():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(nvidia_url)\n",
    "\n",
    "    # get job titles and links for each page and click the next button to go to the next page until no more\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "    time.sleep(1)\n",
    "    next_button = driver.find_element('xpath', '//*[@aria-label=\"next\"]')  \n",
    "    while next_button:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        job_title.extend([td.text for td in soup.findAll(\"a\", {\"data-automation-id\": \"jobTitle\"})])\n",
    "        job_link.extend(['https://nvidia.wd5.myworkdayjobs.com' + td['href'] for td in soup.findAll(\"a\", {\"data-automation-id\": \"jobTitle\"})])\n",
    "        try:\n",
    "            next_button.click()\n",
    "            time.sleep(1)\n",
    "        except: break\n",
    "\n",
    "    # create a dataframe that contains job titles and links for all job categories\n",
    "    df_title_link = pd.DataFrame(zip(job_title, job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_title_link = df_title_link.drop_duplicates()\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = nvidia_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Nvidia', title, link, qual, descrp)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72395e",
   "metadata": {},
   "source": [
    "## Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d59010a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "    \n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        s = ''\n",
    "        d = ''\n",
    "        r = ''\n",
    "        \n",
    "        # job qualifications are inside of the responsibilities section\n",
    "        try:\n",
    "            tag1 = soup.find(\"div\", {'data-bind': \"html: job().responsibilities\"})\n",
    "            \n",
    "            # for responsibilities we retrieve the entire text under tag1\n",
    "            r = tag1.text\n",
    "            \n",
    "            # for qualifications we retrieve the text under 'ul' only\n",
    "            for t in tag1.find_all('ul'): \n",
    "                s = s + ' ' + t.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            # for descriptions we retrieve the entire text under tag2, however, a lot of \n",
    "            # job descriptions are under qualifications section. So we will combined them \n",
    "            tag2 = soup.find(\"div\", {'data-bind': \"html: job().description\"})\n",
    "            d = tag2.text + \" \" + r\n",
    "        except: pass      \n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "521abfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_oracle():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(2)\n",
    "    driver.get(oracle_url)\n",
    "\n",
    "    #have to scroll up and down several times to make load more button visible\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # now click load more button\n",
    "    next_button = driver.find_element('xpath', '//*[@class=\"search-results-load-more-btn\"]')        \n",
    "    while next_button:\n",
    "        next_button.click()\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            next_button = driver.find_element('xpath', '//*[@class=\"search-results-load-more-btn\"]') \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    # scrape job titles and links\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "\n",
    "    job_title.extend(t.text for t in soup.find_all(\"h3\", {\"class\": \"job-title\"}))\n",
    "    job_link.extend(t['href'] for t in soup.find_all(\"a\", {\"class\": \"joblist-tile\"}))\n",
    "\n",
    "\n",
    "    # create a dataframe that contains job titles and links for all job categories\n",
    "    df_title_link = pd.DataFrame(zip(job_title, job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_title_link = df_title_link.drop_duplicates()\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = oracle_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Oracle', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891b2cb",
   "metadata": {},
   "source": [
    "## StateFarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8195d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve job qualifications and descriptions\n",
    "def sf_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):    \n",
    "        s = ''\n",
    "        d = ''\n",
    "        \n",
    "        soup = get_html(driver, link[i])   \n",
    "        \n",
    "        # get qualifications\n",
    "        try:\n",
    "          tag = soup.find(\"strong\", text='Qualifications')\n",
    "          # for some jobs the qualifications are under 'ul' tag as bullet points\n",
    "          # for some other jobs they are under 'p' tag\n",
    "          if tag.findNextSibling(\"ul\"):\n",
    "              s = tag.findNextSibling(\"ul\").text\n",
    "          elif tag.findNextSibling(\"p\"):\n",
    "              s = tag.findNextSibling(\"p\").text\n",
    "        except: pass\n",
    "        \n",
    "        # get responsibilities\n",
    "        try:\n",
    "          tag = soup.find(\"strong\", text='Responsibilities')\n",
    "          # For some jobs the responsibilities are under 'ul' tag as bullet points\n",
    "          # For some other jobs they are under 'p' tag\n",
    "          # When 'ul' is absent from \"Responsibilities\" but presents in \"Qualifications\", \n",
    "          # the code grabs the 'ul' text under qualifications, and this is ok\n",
    "          if tag.findNextSibling(\"ul\"):\n",
    "              d = tag.findNextSibling(\"ul\").text\n",
    "          elif tag.findNextSibling(\"p\"):\n",
    "              d = tag.findNextSibling(\"p\").text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "\n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9f87198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_statefarm():\n",
    "    # retrieve job titles and job links\n",
    "    df_title_link = get_titles_links_byUrl('p', 'class', 'job-title', \n",
    "                                     'a', 'class', 'job-title-link', \n",
    "                                     'https://jobs.statefarm.com',\n",
    "                                      statefarm_url1, statefarm_url2)\n",
    "    print(df_title_link.shape[0])\n",
    "    \n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = sf_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "    \n",
    "    post_process_and_ouput('StateFarm', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab9afea",
   "metadata": {},
   "source": [
    "## Texas Instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce86a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ti_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        try:\n",
    "            soup = get_html(driver, URL)\n",
    "        except:\n",
    "            print(f'ERROR: Failed to load {URL}.')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "\n",
    "        try:\n",
    "          tag = soup.find(re.compile(\"(b|strong)\"), text=re.compile(\"(Minimum Reqirements|Minimum reqirements)\")).findNext(\"ul\")\n",
    "          if tag:\n",
    "            s = s + \" \" + tag.text\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "          tag = soup.find(re.compile(\"(b|strong)\"), text=re.compile(\"(Preferred|Required)\")).findNext(\"ul\")\n",
    "          if tag:\n",
    "            s = s + \" \" + tag.text\n",
    "        except: pass\n",
    "\n",
    "        # retrieve job descriptions. This will only work if the descriptions are listed as \n",
    "        # bullet points under tag \"ul\"\n",
    "        try:\n",
    "          tag = soup.find(\"span\", text=\"Apply online\").findNext('ul')\n",
    "          d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a94d640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_ti():   \n",
    "    # retrieve job titles and job links\n",
    "    df_title_link = get_titles_links_byUrl('div', 'class', 'jobTitle', \n",
    "                                           'a', 'class', 'av-icon-char', 'https://careers.ti.com', \n",
    "                                           ti_url1, ti_url2)\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = ti_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('TI', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67468d59",
   "metadata": {},
   "source": [
    "## Vistra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99067f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve job qualifications and descriptions\n",
    "def vistra_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "\n",
    "        text_pattern_descrp = re.compile(\"Job Description\", re.IGNORECASE)\n",
    "        text_pattern_qual = re.compile(\"Education, Experience, & Skill Requirements|\\\n",
    "                            Key Metrics|\\\n",
    "                            Key Accountabilities (directly or through others)\",\n",
    "                            re.IGNORECASE)\n",
    "        \n",
    "        try:\n",
    "            tag = soup.find(\"\", text=text_pattern_qual).findNext(\"ul\")\n",
    "            s = s + tag.text\n",
    "        except: pass        \n",
    "           \n",
    "        #trys all of the description metrics\n",
    "        try:\n",
    "            tag = soup.find(\"\", text=text_pattern_descrp).findNext(\"ul\")\n",
    "            d = tag.text\n",
    "        except: pass\n",
    "      \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b3dca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_vistra():\n",
    "    # specify the url strings for the company's job posting website\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(vistra_url)\n",
    "\n",
    "    # get job titles and links for each page and click the next button to go to the next page until no more\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "\n",
    "    next_button = driver.find_element('xpath', '//*[@aria-label=\"next\"]')  \n",
    "    while next_button:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        job_title.extend([td.text for td in soup.findAll(\"a\", {\"data-automation-id\": \"jobTitle\"})])\n",
    "        job_link.extend(['https://vst.wd5.myworkdayjobs.com' + td['href'] for td in soup.findAll(\"a\", {\"data-automation-id\": \"jobTitle\"})])\n",
    "        try:\n",
    "            next_button.click()\n",
    "            time.sleep(1)\n",
    "        except: break\n",
    "            \n",
    "    # create a dataframe that contains job titles and links for all job categories\n",
    "    df_title_link = pd.DataFrame(zip(job_title, job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_title_link = df_title_link.drop_duplicates()\n",
    "    print(df_title_link.shape[0])\n",
    "    \n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = vistra_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Vistra', title, link, qual, descrp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b55adc",
   "metadata": {},
   "source": [
    "## Vizient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03762b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve job qualifications and descriptions\n",
    "def vizient_job_description(title, link):\n",
    "    qualifications = []\n",
    "    description = []\n",
    "    jobtitle = []\n",
    "    joblink = []\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "    for i in range(len(link)):\n",
    "        URL=link[i]\n",
    "        driver.get(URL)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        s = ''\n",
    "        d = ''\n",
    "\n",
    "        text_pattern_descrp_1 = re.compile(\"Responsibilities:\", re.IGNORECASE)\n",
    "        \n",
    "        \n",
    "        text_pattern_qual_1 = re.compile(\"Qualifications:\", re.IGNORECASE)        \n",
    "        \n",
    "        \n",
    "        #trys all of the qualification metrics\n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_qual_1).findNext(\"ul\")\n",
    "            s = s + tag.text\n",
    "        except: pass\n",
    "        \n",
    "               \n",
    "        #trys all of the description metrics\n",
    "        try:\n",
    "            tag = soup.find(\"b\", text=text_pattern_descrp_1).findNext(\"ul\")\n",
    "            d = tag.text\n",
    "        except: pass\n",
    "        \n",
    "        qualifications.append(s)\n",
    "        description.append(d)   \n",
    "        jobtitle.append(title[i])\n",
    "        joblink.append(link[i])\n",
    "        \n",
    "    driver.quit()            \n",
    " \n",
    "    return jobtitle, link, qualifications, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf0d1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_vizient():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(vizient_url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # get job titles and links for each page and click the next button to go to the next page until no more\n",
    "    job_title=[]\n",
    "    job_link=[]\n",
    "\n",
    "    next_button = driver.find_element('xpath', '//*[@aria-label=\"next\"]') \n",
    "\n",
    "    while next_button:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        job_title.extend([td.text for td in soup.findAll(\"a\", {\"data-automation-id\": \"jobTitle\"})])\n",
    "        job_link.extend(['https://vizient.wd1.myworkdayjobs.com' + td['href'] for td in soup.findAll(\"a\", {\"data-automation-id\": \"jobTitle\"})])\n",
    "        try:\n",
    "            next_button.click()\n",
    "            time.sleep(1)\n",
    "        except: break\n",
    "\n",
    "    # create a dataframe that contains job titles and links for all job categories\n",
    "    df_title_link = pd.DataFrame(zip(job_title, job_link), columns=['JOB_TITLE', 'JOB_LINK'])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_title_link = df_title_link.drop_duplicates()\n",
    "\n",
    "    print(df_title_link.shape[0])\n",
    "\n",
    "    # retrieve the qualification and descriptions for each job.\n",
    "    title, link, qual, descrp = vizient_job_description(df_title_link['JOB_TITLE'].values[:test], df_title_link['JOB_LINK'].values[:test])\n",
    "\n",
    "    post_process_and_ouput('Vizient', title, link, qual, descrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39d4ef",
   "metadata": {},
   "source": [
    "## Walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75260a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_walmart():\n",
    "    # The jobs page automatically defaults to your location. Therefore, it turns up 0 results.\n",
    "    # The below code will help with making the jobs page not default to a specific locaion.\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    url = 'https://careers.walmart.com/'\n",
    "    driver.get(url)\n",
    "\n",
    "    job_search = driver.find_element(By.XPATH, '//*[@id=\"search\"]')\n",
    "    job_search.send_keys('technology')\n",
    "\n",
    "    search_button = driver.find_element(By.XPATH, '//*[@id=\"location\"]')\n",
    "    search_button.click()\n",
    "\n",
    "    TITLE = []\n",
    "    LINK = []\n",
    "\n",
    "    y = 1\n",
    "    i = 1\n",
    "\n",
    "    while y != 0: \n",
    "        try:        \n",
    "            url = 'https://careers.walmart.com/results?q=&page='+str(i)+'&sort=rank&jobCategory=00000161-7bad-da32-a37b-fbef5e390000,00000161-7bf4-da32-a37b-fbf7c59e0000,00000161-7bff-da32-a37b-fbffc8c10000,00000161-8bd0-d3dd-a1fd-bbd0febc0000,00000161-8be6-da32-a37b-cbe70c150000&jobSubCategory=0000015a-a577-de75-a9ff-bdff284e0000&expand=department,0000015e-b97d-d143-af5e-bd7da8ca0000,00000161-8be6-da32-a37b-cbe70c150000,brand,type,rate&type=jobs'        \n",
    "            driver.get(url) \n",
    "\n",
    "            for j in range(1,26): \n",
    "                job = driver.find_element(By.XPATH, '//*[@id=\"search-results\"]/li['+str(j)+']/div[1]/h4/a')\n",
    "\n",
    "                try:\n",
    "                    TITLE.append(job.text)\n",
    "                except:\n",
    "                    TITLE.append('')\n",
    "\n",
    "                try:\n",
    "                    LINK.append(job.get_attribute('href'))\n",
    "                except:\n",
    "                    LINK.append('')             \n",
    "        except:\n",
    "            y -= 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    driver.close()    \n",
    "    print(len(TITLE), len(LINK)) \n",
    "\n",
    "    df = pd.DataFrame(zip(TITLE, LINK))\n",
    "    df['QUALIFICATIONS'] = np.nan\n",
    "\n",
    "    df.columns = ['TITLE', 'LINK', 'QUALIFICATIONS']\n",
    "\n",
    "    df = df.drop_duplicates(subset=['TITLE'])\n",
    "    print(df.shape[0])\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for i in range(len(df['LINK'])):\n",
    "        try:        \n",
    "            url = (df['LINK'][i])\n",
    "            driver.get(url)\n",
    "            desc = driver.find_element(By.XPATH, '/html/body/main/section[3]/div/div[2]')\n",
    "            df.loc[i,'QUALIFICATIONS'] = desc.text   \n",
    "        except:\n",
    "            df.loc[i,'QUALIFICATIONS'] = np.nan\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    df = df.drop_duplicates(subset=['TITLE', 'LINK', 'QUALIFICATIONS'])\n",
    "    df['QUALIFICATIONS'] = df['QUALIFICATIONS'].str.lower()\n",
    "    df = df.dropna()\n",
    "    df['COMPANY'] = 'Walmart'\n",
    "    df = df.reset_index(drop = True)\n",
    "    len(df)\n",
    "\n",
    "    for i in range(len(df['QUALIFICATIONS'])):\n",
    "        try:\n",
    "            desc = df['QUALIFICATIONS'][i]\n",
    "            mid = desc.index('minimum qualifications')+22\n",
    "            desc = desc[mid:]\n",
    "            df['QUALIFICATIONS'][i] = desc\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # removing unnecessary text\n",
    "    remove1 = '...\\noutlined below are the required minimum qualifications for this position. if none are listed, there are no minimum qualifications.'\n",
    "    remove2 = '...\\noutlined below are the optional preferred qualifications for this position. if none are listed, there are no preferred qualifications.'\n",
    "\n",
    "    for i in range(len(df['QUALIFICATIONS'])):\n",
    "        try:\n",
    "            df['QUALIFICATIONS'][i] = df['QUALIFICATIONS'][i].replace(remove1, '')\n",
    "            df['QUALIFICATIONS'][i] = df['QUALIFICATIONS'][i].replace(remove2, '')\n",
    "            df['QUALIFICATIONS'][i] = df['QUALIFICATIONS'][i].replace('\\n', '')\n",
    "            df['QUALIFICATIONS'][i] = df['QUALIFICATIONS'][i].replace('• ', '')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i in range(len(df['QUALIFICATIONS'])):\n",
    "        try:\n",
    "            desc = df['QUALIFICATIONS'][i]\n",
    "            mid = desc.index('primary location')\n",
    "            desc = desc[:mid]\n",
    "            df.loc[i,'QUALIFICATIONS'] = desc      \n",
    "            df.loc[i,'QUALIFICATIONS'] = df['QUALIFICATIONS'][i].strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df['DESCRIPTION'] = df['QUALIFICATIONS']\n",
    "    df = df[['COMPANY', 'TITLE', 'QUALIFICATIONS', 'LINK', 'DESCRIPTION']]\n",
    "\n",
    "    #df['DESCRIPTION'] = df['QUALIFICATIONS']\n",
    "    df['DESCRIPTION'] = ''\n",
    "    df = df[['COMPANY', 'TITLE', 'QUALIFICATIONS', 'LINK', 'DESCRIPTION']]\n",
    "    print(\"There are {} jobs from Walmart.\".format(df.shape[0]))\n",
    "    df.to_csv('walmart_technology_jobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9eff5",
   "metadata": {},
   "source": [
    "## Main update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "180b17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_update_func(update_list):\n",
    "    start_0 = timer()\n",
    "    if update_list.count('Accenture') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_accenture()\n",
    "            end = timer()\n",
    "            print(f'Accenture: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Accenture scraping failed\\n')\n",
    "            \n",
    "    if update_list.count('Apple') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_apple()\n",
    "            end = timer()\n",
    "            print(f'Apple: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Apple scraping failed\\n')\n",
    "            \n",
    "    if update_list.count('Amazon') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_amazon()\n",
    "            end = timer()\n",
    "            print(f'Amazon: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Amazon scraping failed\\n')\n",
    "\n",
    "    if update_list.count('Deloitte') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_deloitte()\n",
    "            end = timer()\n",
    "            print(f'Deloitte: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Deloitte scraping failed\\n')\n",
    "\n",
    "    if update_list.count('Google') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_google()\n",
    "            end = timer()\n",
    "            print(f'Google: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Google scraping failed\\n')\n",
    "\n",
    "    if update_list.count('IBM') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_ibm()\n",
    "            end = timer()\n",
    "            print(f'IBM: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('IBM scraping failed\\n')\n",
    "\n",
    "    if update_list.count('Intel') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_intel()\n",
    "            end = timer()\n",
    "            print(f'Intel: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Intel scraping failed\\n')\n",
    "\n",
    "    if update_list.count('JnJ') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_jnj()\n",
    "            end = timer()\n",
    "            print(f'JnJ: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('JnJ scraping failed\\n')\n",
    "\n",
    "    if update_list.count('JPM') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_jpm()\n",
    "            end = timer()\n",
    "            print(f'JPM: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('JPM scraping failed\\n')\n",
    "\n",
    "    if update_list.count('KPMG') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_kpmg()\n",
    "            end = timer()\n",
    "            print(f'KPMG: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('KPMG scraping failed\\n')\n",
    "\n",
    "    if update_list.count('Microsoft') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_microsoft()\n",
    "            end = timer()\n",
    "            print(f'Microsoft: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Microsoft scraping failed\\n')\n",
    "\n",
    "\n",
    "    if update_list.count('Nvidia') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_nvidia()\n",
    "            end = timer()\n",
    "            print(f'Nvidia: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Nvidia scraping failed\\n')\n",
    "\n",
    "    if update_list.count('Oracle') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_oracle()\n",
    "            end = timer()\n",
    "            print(f'Oracle: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Oracle scraping failed\\n')\n",
    "\n",
    "\n",
    "    if update_list.count('State Farm') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_statefarm()\n",
    "            end = timer()\n",
    "            print(f'State Farm: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('State Farm scraping failed\\n')\n",
    "\n",
    "\n",
    "    if update_list.count('Texas Instruments') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_ti()\n",
    "            end = timer()\n",
    "            print(f'Texas Instruments: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Texas Instruments scraping failed\\n')\n",
    "\n",
    "\n",
    "    if update_list.count('Cisco') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_cisco()\n",
    "            end = timer()\n",
    "            print(f'Cisco: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Cisco scraping failed\\n')\n",
    "            \n",
    "\n",
    "    if update_list.count('Collabera') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_collabera()\n",
    "            end = timer()\n",
    "            print(f'Collabera: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Collabera scraping failed\\n')\n",
    "\n",
    "\n",
    "    if update_list.count('Expedia') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_expedia()\n",
    "            end = timer()\n",
    "            print(f'Expedia: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Expedia scraping failed\\n')\n",
    "\n",
    "\n",
    "    if update_list.count('Infosys') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_infosys()\n",
    "            end = timer()\n",
    "            print(f'Infosis: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Infosys scraping failed\\n')\n",
    "\n",
    "            \n",
    "    if update_list.count('Walmart') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_walmart()\n",
    "            end = timer()\n",
    "            print(f'Walmart: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Walmart scraping failed\\n')\n",
    "            \n",
    "    if update_list.count('Vizient') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_vizient()\n",
    "            end = timer()\n",
    "            print(f'Vizient: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Vizient scraping failed\\n')\n",
    "\n",
    "    if update_list.count('Vistra') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_vistra()\n",
    "            end = timer()\n",
    "            print(f'Vistra: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Vistra scraping failed\\n')\n",
    "            \n",
    "    if update_list.count('Fox News') > 0:\n",
    "        start = timer()\n",
    "        try:\n",
    "            scrape_jobs_fox()\n",
    "            end = timer()\n",
    "            print(f'Fox News: time elapsed {int((end-start)/60)} minutes\\n')\n",
    "        except:\n",
    "            print('Fox News scraping failed\\n')\n",
    "            \n",
    "    end_0 = timer()\n",
    "    print(f'\\nTotal time elapsed {int((end_0-start_0)/60)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54115cd",
   "metadata": {},
   "source": [
    "# The program expects to have a list of companies for which the jobs will be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c627cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accenture',\n",
       " 'Amazon',\n",
       " 'AppleCisco',\n",
       " 'Collabera',\n",
       " 'Deloitte',\n",
       " 'Expedia',\n",
       " 'Fox NewsGoogle',\n",
       " 'IBM',\n",
       " 'InfosysIntel',\n",
       " 'JnJ',\n",
       " 'JPM',\n",
       " 'KPMG',\n",
       " 'Microsoft',\n",
       " 'Nvidia',\n",
       " 'Oracle',\n",
       " 'State Farm',\n",
       " 'Texas Instruments',\n",
       " 'Vistra',\n",
       " 'Vizient',\n",
       " 'Walmart']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7590f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 9\n",
      "293\n",
      "There are 251 jobs from Accenture.\n",
      "Accenture: time elapsed 22 minutes\n",
      "\n",
      "553\n",
      "There are 552 jobs from Amazon.\n",
      "Amazon: time elapsed 26 minutes\n",
      "\n",
      "1863\n",
      "There are 1765 jobs from Deloitte.\n",
      "Deloitte: time elapsed 120 minutes\n",
      "\n",
      "147\n",
      "There are 133 jobs from IBM.\n",
      "IBM: time elapsed 11 minutes\n",
      "\n",
      "440\n",
      "There are 385 jobs from JnJ.\n",
      "JnJ: time elapsed 22 minutes\n",
      "\n",
      "263\n",
      "There are 92 jobs from JPM.\n",
      "JPM: time elapsed 13 minutes\n",
      "\n",
      "631\n",
      "There are 594 jobs from KPMG.\n",
      "KPMG: time elapsed 36 minutes\n",
      "\n",
      "280\n",
      "There are 269 jobs from Microsoft.\n",
      "Microsoft: time elapsed 9 minutes\n",
      "\n",
      "219\n",
      "There are 208 jobs from Nvidia.\n",
      "Nvidia: time elapsed 6 minutes\n",
      "\n",
      "1051\n",
      "There are 948 jobs from Oracle.\n",
      "Oracle: time elapsed 55 minutes\n",
      "\n",
      "47\n",
      "There are 45 jobs from StateFarm.\n",
      "State Farm: time elapsed 5 minutes\n",
      "\n",
      "397\n",
      "There are 393 jobs from TI.\n",
      "Texas Instruments: time elapsed 27 minutes\n",
      "\n",
      "1476\n",
      "There are 1476 jobs from Collabera.\n",
      "Collabera: time elapsed 53 minutes\n",
      "\n",
      "page 4\n",
      "1\n",
      "Expedia scraping failed\n",
      "\n",
      "363 363\n",
      "220\n",
      "There are 148 jobs from Walmart.\n",
      "Walmart: time elapsed 5 minutes\n",
      "\n",
      "96\n",
      "There are 80 jobs from Vizient.\n",
      "Vizient: time elapsed 2 minutes\n",
      "\n",
      "125\n",
      "There are 77 jobs from Vistra.\n",
      "Vistra: time elapsed 3 minutes\n",
      "\n",
      "\n",
      "Total time elapsed 424 minutes\n"
     ]
    }
   ],
   "source": [
    "#update_list=['Expedia']\n",
    "update_list=complete_company_list\n",
    "main_update_func(update_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
